Step 1: ### Command Execution: `web_search`

```shell
['https://manus.im/share/OZB4PvsXY5N5FrvLXqqCl4?replay=1', 'https://github.com/deepseek-ai', 'https://apidog.com/blog/deepseek-open-source-week/', 'https://github.com/orgs/deepseek-ai/repositories', 'https://api-docs.deepseek.com/news/news250120']

```
Step 2: ### Command Execution: `web_search`

```shell
['https://apidog.com/blog/deepseek-open-source-week/', 'https://apidog.com/blog/deepseek-r1-roocode-ai/', 'https://apidog.com/blog/gemini-2-0-flash-ocr']

```

### Command Execution: `web_search`

```shell
['https://github.com/deepseek-ai', 'https://github.com/deepseek-ai/ESFT', 'https://github.com/deepseek-ai/FlashMLA']

```
Step 3: ### Command Execution: `web_search`

```shell
['https://github.com/deepseek-ai/DeepSeek-Coder', 'https://github.com/deepseek-ai/awesome-deepseek-coder', 'https://apidog.com/blog/deepseek-open-source-week/', 'https://manus.im/share/OZB4PvsXY5N5FrvLXqqCl4?replay=1', 'https://www.reddit.com/r/LocalLLaMA/comments/1ibh9lr/why_deepseek_v3_is_considered_opensource/']

```
Step 4: ### Command Execution: `web_search`

```shell
['https://github.com/deepseek-ai/DeepSeek-V3', 'https://www.reddit.com/r/LocalLLaMA/comments/1ibh9lr/why_deepseek_v3_is_considered_opensource/', 'https://github.com/deepseek-ai/open-infra-index', 'https://www.deepseek.com/', 'https://encord.com/blog/deepseek-ai/']

```
Step 5: ### Command Execution: `str_replace_editor`

```shell
**\1:** \2

```
Step 6: ### Command Execution: `enhanced_browser`

```shell
No text content found on page

```
Step 7: ### Command Execution: `enhanced_browser`

```shell
**\1:** \2

```
Step 8: ### Command Execution: `web_search`

```shell
['https://github.com/deepseek-ai/ESFT', 'https://arxiv.org/html/2407.01906v1', 'https://huggingface.co/deepseek-ai/ESFT-gate-summary-lite']

```
Step 9: ### Command Execution: `bash`

```shell
Windows PowerShell
Copyright (C) Microsoft Corporation. All rights reserved.

Try the new cross-platform PowerShell https://aka.ms/pscore6

PS D:\code\manus\OpenManus\nexagent> curl -s https://raw.githubusercontent.com/deepseek-ai/ESFT/main/README.md; echo '

```
Step 10: ### Command Execution: `bash`

```shell
**\1:** \2
At line:1 char:1
+ curl -s https://raw.githubusercontent.com/deepseek-ai/ESFT/main/READM ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (**\1**) [Invoke-WebRequest], DriveNotFoundExc
   eption
    + FullyQualifiedErrorId : DriveNotFound,Microsoft.PowerShell.Commands.InvokeWebRequestCommand


```
Step 11: ### Command Execution: `bash`

```shell
curl -s https://raw.githubusercontent.com/deepseek-ai/FlashMLA/main/README.md; echo '

```
Step 12: ### Command Execution: `bash`

```shell
**\1:** \2
At line:1 char:1
+ curl -s https://raw.githubusercontent.com/deepseek-ai/FlashMLA/main/R ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (**\1**) [Invoke-WebRequest], DriveNotFoundExc
   eption
    + FullyQualifiedErrorId : DriveNotFound,Microsoft.PowerShell.Commands.InvokeWebRequestCommand


```
Step 13: ### Command Execution: `python_execute`

```python
{'observation': '# FlashMLA\n\nFlashMLA is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable-length sequences serving.\n\nCurrently released:\n- BF16, FP16\n- Paged kvcache with block size of 64\n\n## Quick start\n\n### Install\n\n
```bash\npython setup.py install\n```\n\n### Benchmark\n\n```bash\npython tests/test_flash_mla.py\n```\n\nAchieving up to 3000 GB/s in memory-bound configuration and 580 TFLOPS in computation-bound configuration on H800 SXM5, using CUDA 12.8.\n\n### Usage\n\n```python\nfrom flash_mla import get_mla_metadata, flash_mla_with_kvcache\n\ntile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)\n\nfor i in range(**\1**):\n    ...\n    o_i, lse_i = flash_mla_with_kvcache(\n        q_i, kvcache_i, block_table, cache_seqlens, dv,\n        tile_scheduler_metadata, num_splits, causal=True,\n    )\n    ...\n```\n\n## Requirements\n\n- Hopper GPUs\n- CUDA 12.3 and above\n    - **But we highly recommend 12.8 or above for the best performance**\n- PyTorch 2.0 and above\n\n## Acknowledgement\n\nFlashMLA is inspired by [FlashAttention 2&3](https://github.com/dao-AILab/flash-attention/) and [cutlass](https://github.com/nvidia/cutlass) projects.\n\n## Community Support\n\n### MetaX\nFor MetaX GPUs, visit the official website: [MetaX](**\1**).\n\nThe corresponding FlashMLA version can be found at: [MetaX-MACA/FlashMLA](https://github.com/MetaX-MACA/FlashMLA)\n\n\n### Moore Threads\nFor the Moore Threads GPU, visit the official website: [Moore Threads](**\1**).\n\nThe corresponding FlashMLA version is available on GitHub: [MooreThreads/MT-flashMLA](https://github.com/MooreThreads/MT-flashMLA).\n\n\n### Hygon DCU\nFor the Hygon DCU, visit the official website: [Hygon Developer](https://developer.sourcefind.cn/).\n\nThe corresponding FlashMLA version is available here: [OpenDAS/MLAttention](https://developer.sourcefind.cn/codes/OpenDAS/MLAttention).\n\n\n### Intellifusion\nFor the Intellifusion NNP, visit the official website: [Intellifusion](**\1**).\n\nThe corresponding FlashMLA version is available on Gitee: [Intellifusion/tyllm](https://gitee.com/Intellifusion_2025/tyllm/blob/master/python/tylang/flash_mla.py).\n\n\n### Iluvatar Corex\nFor Iluvatar Corex GPUs, visit the official website: [Iluvatar Corex](**\1**).\n\nThe corresponding FlashMLA version is available on GitHub: [Deep-Spark/FlashMLA](https://github.com/Deep-Spark/FlashMLA/tree/iluvatar_flashmla)\n\n\n### AMD Instinct\nFor AMD Instinct GPUs, visit the official website: [AMD Instinct](https://www.amd.com/en/products/accelerators/instinct.html).\n\nThe corresponding FlashMLA version can be found at: [AITER/MLA](https://github.com/ROCm/aiter/blob/main/aiter/mla.py)\n\n## Citation\n\n```bibtex\n@misc{flashmla2025,\n      title={FlashMLA: Efficient MLA decoding kernels},\n      author={Jiashi Li},\n      year={2025},\n      publisher = {GitHub},\n      howpublished = {\\url{https://github.com/deepseek-ai/FlashMLA}},\n}\n```\n\n', 'success': True, 'error_type': None}
```
Step 14: ### Command Execution: `python_execute`

```python
{'observation': '<p align="center">\n<img width="1000px" alt="DeepSeek Coder" src="pictures/logo.png">\n</p>\n<p align="center"><a href="https://www.deepseek.com/">[<img src="pictures/home.png" width="20px"> Homepage]</a> | <a href="https://coder.deepseek.com/">[ü§ñ Chat with DeepSeek Coder]</a> | <a href="https://huggingface.co/deepseek-ai">[ü§ó Models Download]</a> | <a href="https://discord.gg/Tc7c45Zzu5">[Discord]</a> | <a href="https://github.com/guoday/assert/blob/main/QR.png?raw=true">[WeChat (ÂæÆ‰ø°)]</a></p>\n<p align="center">\n  <a href="https://huggingface.co/papers/2401.14196"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n<hr>\n\n\n### 1. Introduction of DeepSeek Coder\n\nDeepSeek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and an extra fill-in-the-blank task, to support project-level code completion and infilling. For coding capabilities, DeepSeek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\n\n<p align="center">\n<img src="pictures/result.png" alt="result" width="70%">\n</p>\n\n- **Massive Training Data**: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\n\n- **Highly Flexible & Scalable**: Offered in model sizes of 1B, 5.7B, 6.7B and 33B, enabling users to choose the setup most suitable for their requirements.\n\n- **Superior Model Performance**: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\n\n- **Advanced Code Completion Capabilities**: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n\n#### Supported Programming Languages\n`[\'ada\', \'agda\', \'alloy\', \'antlr\', \'applescript\', \'assembly\', \'augeas\', \'awk\', \'batchfile\', \'bluespec\', \'c\', \'c-sharp\', \'clojure\', \'cmake\', \'coffeescript\', \'common-lisp\', \'cpp\', \'css\', \'cuda\', \'dart\', \'dockerfile\', \'elixir\', \'elm\', \'emacs-lisp\', \'erlang\', \'f-sharp\', \'fortran\', \'glsl\', \'go\', \'groovy\', \'haskell\', \'html\', \'idris\', \'isabelle\', \'java\', \'java-server-pages\', \'javascript\', \'json\', \'julia\', \'jupyter-notebook\', \'kotlin\', \'lean\', \'literate-agda\', \'literate-coffeescript\', \'literate-haskell\', \'lua\', \'makefile\', \'maple\', \'markdown\', \'mathematica\', \'matlab\', \'ocaml\', \'pascal\', \'perl\', \'php\', \'powershell\', \'prolog\', \'protocol-buffer\', \'python\', \'r\', \'racket\', \'restructuredtext\', \'rmarkdown\', \'ruby\', \'rust\', \'sas\', \'scala\', \'scheme\', \'shell\', \'smalltalk\', \'solidity\', \'sparql\', \'sql\', \'stan\', \'standard-ml\', \'stata\', \'systemverilog\', \'tcl\', \'tcsh\', \'tex\', \'thrift\', \'typescript\', \'verilog\', \'vhdl\', \'visual-basic\', \'xslt\', \'yacc\', \'yaml\', \'zig\']`\n\n### 2. Evaluation Results\nWe evaluate DeepSeek Coder on various coding-related benchmarks.\nOnly `pass@1` results on HumanEval (**\1**), MBPP, and DS-1000 are reported here:\n\n<p align="center">\n<img src="pictures/table.png" alt="table" width="70%">\n</p>\n\n\nThe result shows that DeepSeek-Coder-Base-33B significantly outperforms existing open-source code LLMs. Compared with CodeLlama-34B, it leads by 7.9%, 9.3%, 10.8% and 5.9% respectively on HumanEval Python, HumanEval Multilingual, MBPP and DS-1000.\nSurprisingly, our DeepSeek-Coder-Base-7B reaches the performance of CodeLlama-34B.\nThe DeepSeek-Coder-Instruct-33B model after instruction tuning outperforms GPT35-turbo on HumanEval and achieves comparable results with GPT35-turbo on MBPP.\n\nMore evaluation details can be found in the [Detailed Evaluation](**\1**).\n\n\n### 3. Procedure of Data Creation and Model Training\n\n#### Data Creation\n\n- Step 1: Collect code data from GitHub and apply the same filtering rules as [StarCoder Data](https://github.com/bigcode-project/bigcode-dataset) to filter data.\n- Step 2: Parsing the dependencies of files within the same repository to rearrange the file positions based on their dependencies.\n- Step 3: Concatenating dependent files to form a single example and employ repo-level minhash for deduplication.\n- Step 4: Further filtering out low-quality code, such as codes with syntax errors or poor readability.\n\n<img src="pictures/data_clean.png" alt="data_creation" width="100%">\n\n#### Model Training\n\n- Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-tra
Step 15: ### Command Execution: `python_execute`

```python
{'observation': '<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/"><img alt="Homepage"\n    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>\n  <a href="https://chat.deepseek.com/"><img alt="Chat"\n    src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>\n  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"\n    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>\n  <br>\n  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"\n    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"\n    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>\n  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"\n    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>\n  <br>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE"><img alt="Code License"\n    src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53"/></a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL"><img alt="Model License"\n    src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53"/></a>\n  <br>\n  <a href="https://arxiv.org/pdf/2412.19437"><b>Paper Link</b>üëÅÔ∏è</a>\n</div>\n\n## Table of Contents\n\n1. [Introduction](**\1**)\n2. [Model Summary](**\1**)\n3. [Model Downloads](**\1**)\n4. [Evaluation Results](**\1**)\n5. [Chat Website & API Platform](**\1**)\n6. [How to Run Locally](**\1**)\n7. [License](**\1**)\n8. [Citation](**\1**)\n9. [Contact](**\1**)\n\n\n## 1. Introduction\n\nWe present DeepSeek-V3, a strong Mixture-of-Experts (**\1**) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (**\1**) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \n<p align="center">\n  <img width="80%" src="figures/benchmark.png">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (**\1**) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration. \n\n---\n\n**Pre-Training: Towards Ultimate Training Efficiency**\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n\n---\n\n**Post-Training: Knowledge Distillation from DeepSeek
Step 16: ### Command Execution: `python_execute`

```python
{'observation': '<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-Open-Infra" />\n</div>\n<hr>\n\n# Hello, DeepSeek Open Infra!\n\n## 202502 Open-Source Week\nWe\'re a tiny team @deepseek-ai pushing our limits in AGI exploration.\n\nStarting **this week** , Feb 24, 2025 we\'ll open-source 5 repos ‚Äì one daily drop ‚Äì not because we\'ve made grand claims, \nbut simply as developers sharing our small-but-sincere progress with full transparency.\n\nThese are humble building blocks of our online service: documented, deployed, and battle-tested in production. \nNo vaporware, just sincere code that moved our tiny yet ambitious dream forward.\n\nWhy? Because every line shared becomes collective momentum that accelerates the journey.\nDaily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation üîß\n\nStay tuned ‚Äì let\'s geek out in the open together.\n\n### Day 1 - [FlashMLA](https://github.com/deepseek-ai/FlashMLA)\n\n**Efficient MLA Decoding Kernel for Hopper GPUs**  \nOptimized for variable-length sequences, battle-tested in production  \n\nüîó [**FlashMLA GitHub Repo**](https://github.com/deepseek-ai/FlashMLA)  \n‚úÖ BF16 support  \n‚úÖ Paged KV cache (**\1**)  \n‚ö° Performance: 3000 GB/s memory-bound | BF16 580 TFLOPS compute-bound on H800\n\n### Day 2 - [DeepEP](https://github.com/deepseek-ai/DeepEP)\n\nExcited to introduce **DeepEP** - the first open-source EP communication library for MoE model training and inference.\n\nüîó [**DeepEP GitHub Repo**](https://github.com/deepseek-ai/DeepEP)  \n‚úÖ Efficient and optimized all-to-all communication  \n‚úÖ Both intranode and internode support with NVLink and RDMA  \n‚úÖ High-throughput kernels for training and inference prefilling  \n‚úÖ Low-latency kernels for inference decoding  \n‚úÖ Native FP8 dispatch support  \n‚úÖ Flexible GPU resource control for computation-communication overlapping  \n\n### Day 3 - [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)\n\nIntroducing **DeepGEMM** - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.\n\nüîó [**DeepGEMM GitHub Repo**](https://github.com/deepseek-ai/DeepGEMM)  \n‚ö° Up to 1350+ FP8 TFLOPS on Hopper GPUs  \n‚úÖ No heavy dependency, as clean as a tutorial  \n‚úÖ Fully Just-In-Time compiled  \n‚úÖ Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes  \n‚úÖ Supports dense layout and two MoE layouts  \n\n### Day 4 - Optimized Parallelism Strategies\n\n‚úÖ **DualPipe** - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.  \nüîó [**GitHub Repo**](https://github.com/deepseek-ai/DualPipe)  \n\n‚úÖ **EPLB** - an expert-parallel load balancer for V3/R1.  \nüîó [**GitHub Repo**](https://github.com/deepseek-ai/eplb)  \n\nüìä Analyze computation-communication overlap in V3/R1.  \nüîó [**GitHub Repo**](https://github.com/deepseek-ai/profile-data)  \n\n### Day 5 - 3FS, Thruster for All DeepSeek Data Access\n\n**Fire-Flyer File System (**\1**)** - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.\n\n‚ö° 6.6 TiB/s aggregate read throughput in a 180-node cluster  \n‚ö° 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster  \n‚ö° 40+ GiB/s peak throughput per client node for KVCache lookup  \nüß¨ Disaggregated architecture with strong consistency semantics  \n‚úÖ Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search & KVCache lookups for inference in V3/R1\n\nüì• **3FS** ‚Üí üîó[**GitHub Repo**](https://github.com/deepseek-ai/3FS)  \n‚õ≤ **Smallpond** - data processing framework on 3FS ‚Üí üîó[**GitHub Repo**](https://github.com/deepseek-ai/smallpond)\n\n### Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview\n\nOptimized throughput and latency via:  \nüîß Cross-node EP-powered batch scaling  \nüîÑ Computation-communication overlap  \n‚öñÔ∏è Load balancing  \n\nProduction data of V3/R1 online services:  \n‚ö° **73.7k/14.8k** input/output tokens per second per H800 node  \nüöÄ Cost profit margin **545%**  \n\n![Cost And Theoretical Income.jpg](202502OpenSourceWeek/figures/Cost%20And%20Theoretical%20Income.jpg)\n\nüí° We hope this week\'s insights offer value to the community and contribute to our shared AGI goals.\n\nüìñ Deep Dive: üîó[Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview](202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md)  \nüìñ ‰∏≠ÊñáÁâà: üîó[DeepSeek-V3 / R1 Êé®ÁêÜÁ≥ªÁªüÊ¶ÇËßà](https://zhuanlan.zhihu.com/p/27181462601)\n\n## 2024 AI Infrastructure Paper (**\1**)  \n### Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning\n\n[**üìÑ Paper Link**](https://dl.acm.org/doi/10.1109/SC41406.2024.00089) 
Step 17: ### Command Execution: `python_execute`

```python
{'observation': "# DeepEP\n\nDeepEP is a communication library tailored for Mixture-of-Experts (**\1**) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.\n\nTo align with the group-limited gating algorithm proposed in the [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (**\1**) number control.\n\nFor latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.\n\nNotice: the implementation in this library may have some slight differences from the [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) paper.\n\n## Performance\n\n### Normal kernels with NVLink and RDMA forwarding\n\nWe test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (**\1**). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).\n\n|   Type    | Dispatch #EP | Bottleneck bandwidth | Combine #EP | Bottleneck bandwidth |\n|:---------:|:------------:|:--------------------:|:-----------:|:--------------------:|\n| Intranode |      8       |  153 GB/s (**\1**)   |      8      |  158 GB/s (**\1**)   |\n| Internode |      16      |    43 GB/s (**\1**)    |     16      |    43 GB/s (**\1**)    |\n| Internode |      32      |    44 GB/s (**\1**)    |     32      |    47 GB/s (**\1**)    |\n| Internode |      64      |    46 GB/s (**\1**)    |     64      |    45 GB/s (**\1**)    |\n\n### Low-latency kernels with pure RDMA\n\nWe test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (**\1**). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).\n\n| Dispatch #EP | Latency | RDMA bandwidth | Combine #EP | Latency | RDMA bandwidth |\n|:------------:|:-------:|:--------------:|:-----------:|:-------:|:--------------:|\n|      8       | 163 us  |    46 GB/s     |      8      | 318 us  |    46 GB/s     |\n|      16      | 173 us  |    43 GB/s     |     16      | 329 us  |    44 GB/s     |\n|      32      | 182 us  |    41 GB/s     |     32      | 350 us  |    41 GB/s     |\n|      64      | 186 us  |    40 GB/s     |     64      | 353 us  |    41 GB/s     |\n|     128      | 192 us  |    39 GB/s     |     128     | 369 us  |    39 GB/s     |\n|     256      | 194 us  |    39 GB/s     |     256     | 360 us  |    40 GB/s     |\n\n## Quick start\n\n### Requirements\n\n- Hopper GPUs (may support more architectures or devices later)\n- Python 3.8 and above\n- CUDA 12.3 and above\n- PyTorch 2.1 and above\n- NVLink for intranode communication\n- RDMA network for internode communication\n\n### Download and install NVSHMEM dependency\n\nDeepEP also depends on our modified NVSHMEM. Please refer to our [NVSHMEM Installation Guide](**\1**) for instructions.\n\n### Development\n\n
```bash\n# Build and make symbolic links for SO files\nNVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build\n# You may modify the specific SO names according to your own platform\nln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so\n\n# Run test cases\n# NOTES: you may modify the `init_dist` function in `tests/utils.py`\n# according to your own cluster settings, and launch into multiple nodes \npython tests/test_intranode.py\npython tests/test_internode.py\npython tests/test_low_latency.py\n```\n\n### Installation\n\n```bash\nNVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install\n```\n\nThen, import `deep_ep` in your Python project, and enjoy!\n\n## Network configurations\n\nDeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (**\1**) as well.\n\n### Traffic isolation\n\nTraffic isolation is supported by InfiniBand through Virtual Lanes (VL).\n\nTo prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:\n\n- workloads using normal kernels\n- workloads using low-latency kernels\n- other workloads\n\nFor DeepEP, you can control the virtual lane assignment by setting the `NVSHMEM_IB_SL` environment variable.\n\n### Adaptive routing\n\nAda
Step 18: ### Command Execution: `terminate`

```shell
The interaction has been completed with status: success

```