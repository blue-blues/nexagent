Step 1: ### Command Execution: `web_search`

```shell
['/search?num=7', 'https://manus.im/share/OZB4PvsXY5N5FrvLXqqCl4?replay=1', 'https://github.com/deepseek-ai', 'https://apidog.com/blog/deepseek-open-source-week/', 'https://github.com/orgs/deepseek-ai/repositories']

```
Step 2: ### Command Execution: `enhanced_browser`

```shell
No text content found on page

```
Step 3: ### Command Execution: `enhanced_browser`

```shell
**\1:** \2

```
Step 4: ### Command Execution: `enhanced_browser`

```shell
**\1:** \2

```
Step 5: ### Command Execution: `web_search`

```shell
['https://github.com/orgs/deepseek-ai/repositories', 'https://github.com/deepseek-ai', 'https://api-docs.deepseek.com/news/news250120', 'https://medium.com/@345490675/day-5-of-deepseeks-open-source-week-3fs-the-data-highway-in-the-ai-realm-2bdebff53b56', 'https://analyticsindiamag.com/ai-news-updates/deepseek-will-announce-5-open-source-repositories-starting-next-week/']

```
Step 6: ### Command Execution: `enhanced_browser`

```shell
**\1:** \2

```
Step 7: ### Command Execution: `web_search`

```shell
['https://github.com/deepseek-ai/3FS', 'https://github.com/deepseek-ai', 'https://github.com/orgs/deepseek-ai/repositories']

```
Step 8: ### Command Execution: `bash`

```shell
Windows PowerShell
Copyright (C) Microsoft Corporation. All rights reserved.

Try the new cross-platform PowerShell https://aka.ms/pscore6

PS D:\code\manus\OpenManus\nexagent> curl https://raw.githubusercontent.com/deepseek-ai/3FS/main/README.md; echo '

```
Step 9: ### Command Execution: `python_execute`

```python
{'observation': '#  Fire-Flyer File System\n\n[![Build](https://github.com/deepseek-ai/3fs/actions/workflows/build.yml/badge.svg)](https://github.com/deepseek-ai/3fs/actions/workflows/build.yml)\n[![License](https://img.shields.io/badge/LICENSE-MIT-blue.svg)](**\1**)\n\nThe Fire-Flyer File System (**\1**) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\n\n- Performance and Usability\n  - **Disaggregated Architecture** Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\n  - **Strong Consistency** Implements Chain Replication with Apportioned Queries (**\1**) for strong consistency, making application code simple and easy to reason about.\n  - **File Interfaces** Develops stateless metadata services backed by a transactional key-value store (**\1**). The file interface is well known and used everywhere. There is no need to learn a new storage API.\n\n- Diverse Workloads\n  - **Data Preparation** Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\n  - **Dataloaders** Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\n  - **Checkpointing** Supports high-throughput parallel checkpointing for large-scale training.\n  - **KVCache for Inference** Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\n\n## Documentation\n\n* [Design Notes](**\1**)\n* [Setup Guide](**\1**)\n* [USRBIO API Reference](**\1**)\n* [P Specifications](**\1**)\n\n## Performance\n\n### 1. Peak throughput\n\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\n\n![Large block read throughput under stress test on a 180-node cluster](docs/images/peak_throughput.jpg)\n\nTo benchmark 3FS, please use our [fio engine for USRBIO](benchmarks/fio_usrbio/README.md).\n\n### 2. GraySort\n\nWe evaluated [smallpond](https://github.com/deepseek-ai/smallpond) using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\n\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of *3.66 TiB/min*.\n\n![](docs/images/gray_sort_server.png)\n![](docs/images/gray_sort_client.png)\n\n### 3. KVCache\n\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers.\nThe top figure demonstrates the read throughput of all KVCache clients (**\1**), highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.\n\n![KVCache Read Throughput](./docs/images/kvcache_read_throughput.png)\n![KVCache GC IOPS](./docs/images/kvcache_gc_iops.png)\n\n## Check out source code\n\nClone 3FS repository from GitHub:\n\n\tgit clone https://github.com/deepseek-ai/3fs\n\nWhen `deepseek-ai/3fs` has been cloned to a local file system, run the\nfollowing commands to check out the submodules:\n\n
```bash\ncd 3fs\ngit submodule update --init --recursive\n./patches/apply.sh\n```\n\n## Install dependencies\n\nInstall dependencies:\n\n```bash\n# for Ubuntu 20.04.\napt install cmake libuv1-dev liblz4-dev liblzma-dev libdouble-conversion-dev libdwarf-dev libunwind-dev \\\n  libaio-dev libgflags-dev libgoogle-glog-dev libgtest-dev libgmock-dev clang-format-14 clang-14 clang-tidy-14 lld-14 \\\n  libgoogle-perftools-dev google-perftools libssl-dev libclang-rt-14-dev gcc-10 g++-10 libboost1.71-all-dev build-essential\n\n# for Ubuntu 22.04.\napt install cmake li
Step 10: ### Command Execution: `enhanced_browser`

```shell
No text content found on page

```
Step 11: ### Command Execution: `bash`

```shell


StatusCode        : 200
StatusDescription : OK
Content           : #  Fire-Flyer File System

                    [![Build](https://github.com/deepseek-ai/3fs/actions/workflows/build.yml/badge.svg)](https://github.com/deepseek-ai/3fs/actions/wor
                    kflows/build.yml)
                    [![License](https://img....
RawContent        : HTTP/1.1 200 OK
                    Connection: keep-alive
                    Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
                    Strict-Transport-Security: max-age=31536000
                    X-Content-Type-Options: nosniff
                    ...
Forms             : {}
Headers           : {[Connection, keep-alive], [Content-Security-Policy, default-src 'none'; style-src 'unsafe-inline'; sandbox],
                    [Strict-Transport-Security, max-age=31536000], [X-Content-Type-Options, nosniff]...}
Images            : {}
InputFields       : {}
Links             : {}
ParsedHtml        : mshtml.HTMLDocumentClass
RawContentLength  : 6905


```
Step 12: ### Command Execution: `web_search`

```shell
['https://apidog.com/blog/deepseek-open-source-week/', 'https://github.com/deepseek-ai/open-infra-index', 'https://medium.com/@ai-data-drive/what-lies-behind-deepseeks-open-source-week-deep-research-uncovers-it-all-4d7fe583c907', 'https://dev.to/apilover/deepseek-open-source-week-kicked-off-with-flashmlagithub-codebase-included-53im', 'https://medium.com/towards-agi/deepseek-day-2-of-open-source-week-exploring-deepep-9abd7b3d096f']

```
Step 13: ### Command Execution: `enhanced_browser`

```shell
No text content found on page

```
Step 14: ### Command Execution: `web_search`

```shell
['https://github.com/deepseek-ai/open-infra-index', 'https://github.com/deepseek-ai', 'https://www.linkedin.com/posts/upgundecha_github-deepseek-aiopen-infra-index-production-tested-activity-7301888671002046465-2OIO']

```
Step 15: ### Command Execution: `python_execute`

```python
{'observation': '<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-Open-Infra" />\n</div>\n<hr>\n\n# Hello, DeepSeek Open Infra!\n\n## 202502 Open-Source Week\nWe\'re a tiny team @deepseek-ai pushing our limits in AGI exploration.\n\nStarting **this week** , Feb 24, 2025 we\'ll open-source 5 repos – one daily drop – not because we\'ve made grand claims, \nbut simply as developers sharing our small-but-sincere progress with full transparency.\n\nThese are humble building blocks of our online service: documented, deployed, and battle-tested in production. \nNo vaporware, just sincere code that moved our tiny yet ambitious dream forward.\n\nWhy? Because every line shared becomes collective momentum that accelerates the journey.\nDaily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation 🔧\n\nStay tuned – let\'s geek out in the open together.\n\n### Day 1 - [FlashMLA](https://github.com/deepseek-ai/FlashMLA)\n\n**Efficient MLA Decoding Kernel for Hopper GPUs**  \nOptimized for variable-length sequences, battle-tested in production  \n\n🔗 [**FlashMLA GitHub Repo**](https://github.com/deepseek-ai/FlashMLA)  \n✅ BF16 support  \n✅ Paged KV cache (**\1**)  \n⚡ Performance: 3000 GB/s memory-bound | BF16 580 TFLOPS compute-bound on H800\n\n### Day 2 - [DeepEP](https://github.com/deepseek-ai/DeepEP)\n\nExcited to introduce **DeepEP** - the first open-source EP communication library for MoE model training and inference.\n\n🔗 [**DeepEP GitHub Repo**](https://github.com/deepseek-ai/DeepEP)  \n✅ Efficient and optimized all-to-all communication  \n✅ Both intranode and internode support with NVLink and RDMA  \n✅ High-throughput kernels for training and inference prefilling  \n✅ Low-latency kernels for inference decoding  \n✅ Native FP8 dispatch support  \n✅ Flexible GPU resource control for computation-communication overlapping  \n\n### Day 3 - [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM)\n\nIntroducing **DeepGEMM** - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3/R1 training and inference.\n\n🔗 [**DeepGEMM GitHub Repo**](https://github.com/deepseek-ai/DeepGEMM)  \n⚡ Up to 1350+ FP8 TFLOPS on Hopper GPUs  \n✅ No heavy dependency, as clean as a tutorial  \n✅ Fully Just-In-Time compiled  \n✅ Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes  \n✅ Supports dense layout and two MoE layouts  \n\n### Day 4 - Optimized Parallelism Strategies\n\n✅ **DualPipe** - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.  \n🔗 [**GitHub Repo**](https://github.com/deepseek-ai/DualPipe)  \n\n✅ **EPLB** - an expert-parallel load balancer for V3/R1.  \n🔗 [**GitHub Repo**](https://github.com/deepseek-ai/eplb)  \n\n📊 Analyze computation-communication overlap in V3/R1.  \n🔗 [**GitHub Repo**](https://github.com/deepseek-ai/profile-data)  \n\n### Day 5 - 3FS, Thruster for All DeepSeek Data Access\n\n**Fire-Flyer File System (**\1**)** - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks.\n\n⚡ 6.6 TiB/s aggregate read throughput in a 180-node cluster  \n⚡ 3.66 TiB/min throughput on GraySort benchmark in a 25-node cluster  \n⚡ 40+ GiB/s peak throughput per client node for KVCache lookup  \n🧬 Disaggregated architecture with strong consistency semantics  \n✅ Training data preprocessing, dataset loading, checkpoint saving/reloading, embedding vector search & KVCache lookups for inference in V3/R1\n\n📥 **3FS** → 🔗[**GitHub Repo**](https://github.com/deepseek-ai/3FS)  \n⛲ **Smallpond** - data processing framework on 3FS → 🔗[**GitHub Repo**](https://github.com/deepseek-ai/smallpond)\n\n### Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview\n\nOptimized throughput and latency via:  \n🔧 Cross-node EP-powered batch scaling  \n🔄 Computation-communication overlap  \n⚖️ Load balancing  \n\nProduction data of V3/R1 online services:  \n⚡ **73.7k/14.8k** input/output tokens per second per H800 node  \n🚀 Cost profit margin **545%**  \n\n![Cost And Theoretical Income.jpg](202502OpenSourceWeek/figures/Cost%20And%20Theoretical%20Income.jpg)\n\n💡 We hope this week\'s insights offer value to the community and contribute to our shared AGI goals.\n\n📖 Deep Dive: 🔗[Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview](202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md)  \n📖 中文版: 🔗[DeepSeek-V3 / R1 推理系统概览](https://zhuanlan.zhihu.com/p/27181462601)\n\n## 2024 AI Infrastructure Paper (**\1**)  \n### Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning\n\n[**📄 Paper Link**](https://dl.acm.org/doi/10.1109/SC41406.2024.00089) 
Step 16: ### Command Execution: `python_execute`

```python
{'observation': '# FlashMLA\n\nFlashMLA is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable-length sequences serving.\n\nCurrently released:\n- BF16, FP16\n- Paged kvcache with block size of 64\n\n## Quick start\n\n### Install\n\n
```bash\npython setup.py install\n```\n\n### Benchmark\n\n```bash\npython tests/test_flash_mla.py\n```\n\nAchieving up to 3000 GB/s in memory-bound configuration and 580 TFLOPS in computation-bound configuration on H800 SXM5, using CUDA 12.8.\n\n### Usage\n\n```python\nfrom flash_mla import get_mla_metadata, flash_mla_with_kvcache\n\ntile_scheduler_metadata, num_splits = get_mla_metadata(cache_seqlens, s_q * h_q // h_kv, h_kv)\n\nfor i in range(**\1**):\n    ...\n    o_i, lse_i = flash_mla_with_kvcache(\n        q_i, kvcache_i, block_table, cache_seqlens, dv,\n        tile_scheduler_metadata, num_splits, causal=True,\n    )\n    ...\n```\n\n## Requirements\n\n- Hopper GPUs\n- CUDA 12.3 and above\n    - **But we highly recommend 12.8 or above for the best performance**\n- PyTorch 2.0 and above\n\n## Acknowledgement\n\nFlashMLA is inspired by [FlashAttention 2&3](https://github.com/dao-AILab/flash-attention/) and [cutlass](https://github.com/nvidia/cutlass) projects.\n\n## Community Support\n\n### MetaX\nFor MetaX GPUs, visit the official website: [MetaX](**\1**).\n\nThe corresponding FlashMLA version can be found at: [MetaX-MACA/FlashMLA](https://github.com/MetaX-MACA/FlashMLA)\n\n\n### Moore Threads\nFor the Moore Threads GPU, visit the official website: [Moore Threads](**\1**).\n\nThe corresponding FlashMLA version is available on GitHub: [MooreThreads/MT-flashMLA](https://github.com/MooreThreads/MT-flashMLA).\n\n\n### Hygon DCU\nFor the Hygon DCU, visit the official website: [Hygon Developer](https://developer.sourcefind.cn/).\n\nThe corresponding FlashMLA version is available here: [OpenDAS/MLAttention](https://developer.sourcefind.cn/codes/OpenDAS/MLAttention).\n\n\n### Intellifusion\nFor the Intellifusion NNP, visit the official website: [Intellifusion](**\1**).\n\nThe corresponding FlashMLA version is available on Gitee: [Intellifusion/tyllm](https://gitee.com/Intellifusion_2025/tyllm/blob/master/python/tylang/flash_mla.py).\n\n\n### Iluvatar Corex\nFor Iluvatar Corex GPUs, visit the official website: [Iluvatar Corex](**\1**).\n\nThe corresponding FlashMLA version is available on GitHub: [Deep-Spark/FlashMLA](https://github.com/Deep-Spark/FlashMLA/tree/iluvatar_flashmla)\n\n\n### AMD Instinct\nFor AMD Instinct GPUs, visit the official website: [AMD Instinct](https://www.amd.com/en/products/accelerators/instinct.html).\n\nThe corresponding FlashMLA version can be found at: [AITER/MLA](https://github.com/ROCm/aiter/blob/main/aiter/mla.py)\n\n## Citation\n\n```bibtex\n@misc{flashmla2025,\n      title={FlashMLA: Efficient MLA decoding kernels},\n      author={Jiashi Li},\n      year={2025},\n      publisher = {GitHub},\n      howpublished = {\\url{https://github.com/deepseek-ai/FlashMLA}},\n}\n```\n\n', 'success': True, 'error_type': None}
```
Step 17: ### Command Execution: `python_execute`

```python
{'observation': '⚠️ Execution timeout after 5 seconds. Your code may contain an infinite loop or is taking too long to execute.', 'success': False, 'error_type': 'TimeoutError'}

```
Step 18: ### Command Execution: `python_execute`

```python
{'observation': "# DeepEP\n\nDeepEP is a communication library tailored for Mixture-of-Experts (**\1**) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.\n\nTo align with the group-limited gating algorithm proposed in the [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (**\1**) number control.\n\nFor latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.\n\nNotice: the implementation in this library may have some slight differences from the [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) paper.\n\n## Performance\n\n### Normal kernels with NVLink and RDMA forwarding\n\nWe test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (**\1**). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).\n\n|   Type    | Dispatch #EP | Bottleneck bandwidth | Combine #EP | Bottleneck bandwidth |\n|:---------:|:------------:|:--------------------:|:-----------:|:--------------------:|\n| Intranode |      8       |  153 GB/s (**\1**)   |      8      |  158 GB/s (**\1**)   |\n| Internode |      16      |    43 GB/s (**\1**)    |     16      |    43 GB/s (**\1**)    |\n| Internode |      32      |    44 GB/s (**\1**)    |     32      |    47 GB/s (**\1**)    |\n| Internode |      64      |    46 GB/s (**\1**)    |     64      |    45 GB/s (**\1**)    |\n\n### Low-latency kernels with pure RDMA\n\nWe test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (**\1**). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).\n\n| Dispatch #EP | Latency | RDMA bandwidth | Combine #EP | Latency | RDMA bandwidth |\n|:------------:|:-------:|:--------------:|:-----------:|:-------:|:--------------:|\n|      8       | 163 us  |    46 GB/s     |      8      | 318 us  |    46 GB/s     |\n|      16      | 173 us  |    43 GB/s     |     16      | 329 us  |    44 GB/s     |\n|      32      | 182 us  |    41 GB/s     |     32      | 350 us  |    41 GB/s     |\n|      64      | 186 us  |    40 GB/s     |     64      | 353 us  |    41 GB/s     |\n|     128      | 192 us  |    39 GB/s     |     128     | 369 us  |    39 GB/s     |\n|     256      | 194 us  |    39 GB/s     |     256     | 360 us  |    40 GB/s     |\n\n## Quick start\n\n### Requirements\n\n- Hopper GPUs (may support more architectures or devices later)\n- Python 3.8 and above\n- CUDA 12.3 and above\n- PyTorch 2.1 and above\n- NVLink for intranode communication\n- RDMA network for internode communication\n\n### Download and install NVSHMEM dependency\n\nDeepEP also depends on our modified NVSHMEM. Please refer to our [NVSHMEM Installation Guide](**\1**) for instructions.\n\n### Development\n\n
```bash\n# Build and make symbolic links for SO files\nNVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build\n# You may modify the specific SO names according to your own platform\nln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so\n\n# Run test cases\n# NOTES: you may modify the `init_dist` function in `tests/utils.py`\n# according to your own cluster settings, and launch into multiple nodes \npython tests/test_intranode.py\npython tests/test_internode.py\npython tests/test_low_latency.py\n```\n\n### Installation\n\n```bash\nNVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install\n```\n\nThen, import `deep_ep` in your Python project, and enjoy!\n\n## Network configurations\n\nDeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (**\1**) as well.\n\n### Traffic isolation\n\nTraffic isolation is supported by InfiniBand through Virtual Lanes (VL).\n\nTo prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:\n\n- workloads using normal kernels\n- workloads using low-latency kernels\n- other workloads\n\nFor DeepEP, you can control the virtual lane assignment by setting the `NVSHMEM_IB_SL` environment variable.\n\n### Adaptive routing\n\nAda
Step 19: ### Command Execution: `python_execute`

```python
{'observation': '# DeepGEMM\n\nDeepGEMM is a library designed for clean and efficient FP8 General Matrix Multiplications (**\1**) with fine-grained scaling, as proposed in [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3). It supports both normal and Mix-of-Experts (**\1**) grouped GEMMs. Written in CUDA, the library has no compilation need during installation, by compiling all kernels at runtime using a lightweight Just-In-Time (**\1**) module.\n\nCurrently, DeepGEMM exclusively supports NVIDIA Hopper tensor cores. To address the imprecise FP8 tensor core accumulation, it employs CUDA-core two-level accumulation (**\1**). While it leverages some concepts from [CUTLASS](https://github.com/nvidia/cutlass) and [CuTe](https://github.com/NVIDIA/cutlass/tree/main/include/cute), it avoids heavy reliance on their templates or algebras. Instead, the library is designed for simplicity, with only one core kernel function comprising around **~300 lines of code**. This makes it a clean and accessible resource for learning Hopper FP8 matrix multiplication and optimization techniques.\n\nDespite its lightweight design, DeepGEMM\'s performance matches or exceeds expert-tuned libraries across various matrix shapes.\n\n## Performance\n\nWe test all shapes potentially used in DeepSeek-V3/R1 inference (including both prefilling and decoding, but without tensor parallelism) on H800 SXM5 with NVCC 12.8. All speedup metrics are calculated in comparison to our internally and carefully optimized implementation based on CUTLASS 3.6.\n\nDeepGEMM does not behave very well on some shapes, optimization PRs are welcomed if you are interested.\n\n### Normal GEMMs for dense models\n\n|  M   |   N   |   K   | Computation | Memory bandwidth | Speedup |\n|:----:|:-----:|:-----:|:-----------:|:----------------:|:-------:|\n|  64  | 2112  | 7168  | 206 TFLOPS  |    1688 GB/s     |  2.7x   |\n|  64  | 24576 | 1536  | 289 TFLOPS  |    2455 GB/s     |  1.7x   |\n|  64  | 32768 |  512  | 219 TFLOPS  |    2143 GB/s     |  1.8x   |\n|  64  | 7168  | 16384 | 336 TFLOPS  |    2668 GB/s     |  1.4x   |\n|  64  | 4096  | 7168  | 287 TFLOPS  |    2320 GB/s     |  1.4x   |\n|  64  | 7168  | 2048  | 295 TFLOPS  |    2470 GB/s     |  1.7x   |\n| 128  | 2112  | 7168  | 352 TFLOPS  |    1509 GB/s     |  2.4x   |\n| 128  | 24576 | 1536  | 535 TFLOPS  |    2448 GB/s     |  1.6x   |\n| 128  | 32768 |  512  | 358 TFLOPS  |    2103 GB/s     |  1.5x   |\n| 128  | 7168  | 16384 | 645 TFLOPS  |    2604 GB/s     |  1.4x   |\n| 128  | 4096  | 7168  | 533 TFLOPS  |    2221 GB/s     |  2.0x   |\n| 128  | 7168  | 2048  | 510 TFLOPS  |    2277 GB/s     |  1.7x   |\n| 4096 | 2112  | 7168  | 1009 TFLOPS |     503 GB/s     |  1.1x   |\n| 4096 | 24576 | 1536  | 1125 TFLOPS |     893 GB/s     |  1.1x   |\n| 4096 | 32768 |  512  | 751 TFLOPS  |    1569 GB/s     |  1.1x   |\n| 4096 | 7168  | 16384 | 1426 TFLOPS |     361 GB/s     |  1.3x   |\n| 4096 | 4096  | 7168  | 1265 TFLOPS |     485 GB/s     |  1.2x   |\n| 4096 | 7168  | 2048  | 1168 TFLOPS |     794 GB/s     |  1.2x   |\n\n### Grouped GEMMs for MoE models (**\1**)\n\n| #Groups | M per group |  N   |  K   | Computation | Memory bandwidth | Speedup |\n|:-------:|:-----------:|:----:|:----:|:-----------:|:----------------:|:-------:|\n|    4    |    8192     | 4096 | 7168 | 1346 TFLOPS |     434 GB/s     |  1.3x   |\n|    4    |    8192     | 7168 | 2048 | 1214 TFLOPS |     752 GB/s     |  1.3x   |\n|    8    |    4096     | 4096 | 7168 | 1346 TFLOPS |     516 GB/s     |  1.3x   |\n|    8    |    4096     | 7168 | 2048 | 1214 TFLOPS |     826 GB/s     |  1.2x   |\n\n### Grouped GEMMs for MoE models (**\1**)\n\n| #Groups | M per group |  N   |  K   | Computation | Memory bandwidth | Speedup |\n|:-------:|:-----------:|:----:|:----:|:-----------:|:----------------:|:-------:|\n|    1    |    1024     | 4096 | 7168 | 1233 TFLOPS |     924 GB/s     |  1.2x   |\n|    1    |    1024     | 7168 | 2048 | 925 TFLOPS  |     968 GB/s     |  1.2x   |\n|    2    |     512     | 4096 | 7168 | 1040 TFLOPS |    1288 GB/s     |  1.2x   |\n|    2    |     512     | 7168 | 2048 | 916 TFLOPS  |    1405 GB/s     |  1.2x   |\n|    4    |     256     | 4096 | 7168 | 932 TFLOPS  |    2064 GB/s     |  1.1x   |\n|    4    |     256     | 7168 | 2048 | 815 TFLOPS  |    2047 GB/s     |  1.2x   |\n\n## Quick start\n\n### Requirements\n\n- Hopper architecture GPUs, `sm_90a` must be supported\n- Python 3.8 or above\n- CUDA 12.3 or above\n  - **But we highly recommend 12.8 or above for the best performance**\n- PyTorch 2.1 or above\n- CUTLASS 3.6 or above (could be cloned by Git submodule)\n\n### Development\n\n
```bash\n# Submodule must be cloned\ngit clone --recursive git@github.com:deepseek-ai/DeepGEMM.git\n\n# Make symbolic links for third-party (**\1**) include directories\npython setup.py develop\n\n# Test JIT compilation\npython tests/test_jit.py\n\n# Test all GEMM impl
Step 20: ### Command Execution: `python_execute`

```python
{'observation': '# DualPipe\n\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the [DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437). It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the [profile data](https://github.com/deepseek-ai/profile-data).\n\n### Schedules\n\n![dualpipe](**\1**)\n\nExample DualPipe scheduling for 8 PP ranks and 20 micro-batches in two directions.\nThe micro-batches in the reverse direction are symmetric to those in the forward direction, so\nwe omit their batch ID for illustration simplicity. Two cells enclosed by a shared black border\nhave mutually overlapped computation and communication\n\n## DualPipeV\n\nDualPipeV is a concise V-shape schedule derived from DualPipe using a "cut-in-half" procedure, introduced by Sea AI Lab as "Cut-in-half" in their [blog post](https://hackmd.io/@ufotalent/r1lVXsa9Jg). Thanks to them for this efficient schedule!\n\n### Schedules\n\n![dualpipev](**\1**)\n\nExample DualPipeV scheduling for 4 PP ranks (**\1**) and 10 micro-batches.\n\n## Pipeline Bubbles and Memory Usage Comparison (based on the same number of PP stages)\n\n| Method      | Bubble                          | Parameter Per Device | Activation Per Device | #Devices |\n|-------------|---------------------------------|----------------------|-----------------------|----------|\n| 1F1B        | (**\1**)(**\1**)                   | 1×                   | *PP*                  | *PP*     |\n| ZB1P        | (**\1**)(**\1**)               | 1×                   | *PP*                  | *PP*     |\n| DualPipe    | (**\1**)(**\1**)           | 2×                   | *PP*+1                | *PP*     |\n| DualPipeV   | (**\1**)(**\1**)           | 2×                   | *PP*+1                | *PP*/2   |\n\n*PP* denotes the number of pp stages (**\1**).\n𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a\nfull backward chunk, 𝑊 denotes the execution time of a "backward for weights" chunk, and 𝐹&𝐵\ndenotes the execution time of two mutually overlapped forward and backward chunks.\n\n## Quick Start\n\nThe usage is shown in the following example:\n\n
```bash\npython examples/example_dualpipe.py\npython examples/example_dualpipev.py\n```\n\n**\1:** \2
```
Step 21: ### Command Execution: `python_execute`

```python
{'observation': "# Expert Parallelism Load Balancer (**\1**)\n\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different \nexperts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. \nAs described in the DeepSeek-V3 paper, we adopt a **redundant experts** strategy that duplicates heavy-loaded experts. \nThen, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, \nthanks to the **group-limited expert routing** used in DeepSeek-V3, we also attempt to place the experts of the same \ngroup to the same node to reduce inter-node data traffic, whenever possible.\n\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in `eplb.py`. \nThe algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note \nthat the exact method to predict the loads of experts is out of this repo's scope. A common method is to use \nmoving average of historical statistics. \n\n## The Algorithm\n\nThe load balancing algorithm comes with two policies used for different cases.\n\n### Hierarchical Load Balancing\n\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to\nharness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of \ndifferent nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts \nto individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in \nprefilling stage with a smaller expert-parallel size.\n\n### Global Load Balancing\n\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert \ngroups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger \nexpert-parallel size.\n\n## Interface and Example\n\nThe main function of the load balancer is `eplb.rebalance_experts`.\n\nThe following code illustrates an example of a two-layer MoE model, and each layer contains 12 experts. We introduce 4 redundant experts per layer, and the total 16 replicas are placed on 2 nodes, and each node contains 4 GPUs.\n\n
``` python\nimport torch\nimport eplb\n\nweight = torch.tensor([[ 90, 132,  40,  61, 104, 165,  39,   4,  73,  56, 183,  86],\n                       [ 20, 107, 104,  64,  19, 197, 187, 157, 172,  86,  16,  27]])\n\nnum_replicas = 16\nnum_groups = 4\nnum_nodes = 2\nnum_gpus = 8\n\nphy2log, log2phy, logcnt = eplb.rebalance_experts(weight, num_replicas, num_groups, num_nodes, num_gpus)\nprint(**\1**)\n\n# Output:\n# tensor([[ 5,  6,  5,  7,  8,  4,  3,  4, 10,  9, 10,  2,  0,  1, 11,  1],\n#         [ 7, 10,  6,  8,  6, 11,  8,  9,  2,  4,  5,  1,  5,  0,  3,  1]])\n```\n\nThe output, generated by the hierarchical load balancing policy, indicates the following \nexpert replication and placement plan.\n\n![](**\1**)\n\n\n## License\n\nThis code repository is released under the MIT License.\n\n", 'success': True, 'error_type': None}
```
Step 22: ### Command Execution: `terminate`

```shell
The interaction has been completed with status: success

```